# Numerical-Optimization

from scratch without using any python libraries

1) developed a Python program that is able to implement the gradient descent in order to achieve the linear regression of a set of datapoints.

2) implemented the gradient descent variants (Batch/Mini-Batch/Stochastic) in order to achieve the linear regression of a set of datapoints.

3) implemented the accelerated gradient descent methods (Momentum and NAG) in order to achieve the linear regression of a set of datapoints.
 
4) implemented the accelerated gradient descent methods with adaptive learning rate (Adagrad, RMSProp, and Adam) in order to achieve the linear regression of a set of datapoints
